# -*- coding: utf-8 -*-
"""Bank_Personal_Loan_Modelling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YsUZT08u7pUmYlc8zx9k-BwDND5ErWEA
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import io
import matplotlib.pyplot as plt
# %matplotlib inline

from google.colab import files
data = files.upload()

df = pd.read_csv(io.BytesIO(data["Data.csv"]))

df.head()

print("There are {} rows and {} columns.".format(df.shape[0], df.shape[1]))

df.info()

print("Checking for null values in our the dataset")
df.isna().any()

"""### ID and Zipcode might be removed as they may not be useful for our analysis"""

df.drop(['ID', 'ZIP Code'], axis = 1, inplace = True)

df.describe().T

df.boxplot(column=['Age', 'Experience', 'Income', 'Family', 'Education'], return_type='axes', figsize=(10, 8))

"""### Income has too much noise and slightly skewed right, Age and exp are equally distributed."""

df.skew(numeric_only=True)

"""### we could see that CCAvg,Mortage,Personal Loan, Securities account,CD account are highly skewed"""

columns = list(df)
df[columns].hist(stacked = True, density = True, bins = 100, color = 'orange', figsize = (16, 30), layout = (10, 3))

"""*   Age & Experience are to an extent equally distributed
*   60% of customers have enabled online banking and went digital
*   Income & Credit card spending are skewed to the left
"""

import seaborn as sns

sns.distplot(df['Age'],kde=True,hist=False,color='Red')
sns.distplot(df['Income'],kde=True,hist=False,color='Green')
sns.distplot(df['Experience'],kde=True,hist=False,color='blue')
plt.show()

corr = df.corr()
sns.heatmap(corr)

"""### We could see that Age & Experience are very strongly correlated, Hence it is fine for us to go with Age and drop Experience to avoid multi-colinearity issue."""

df=df.drop(['Experience'],axis=1)

def edu(row):    
    if row['Education']==1:
        return "Undergrad"
    elif row['Education']==2:
        return "Graduate"
    else:
        return "Advanced/Professional"
df['EDU']=df.apply(edu,axis=1)

EDU_dis=df.groupby('EDU')["Age"].count()
EDU_dis.plot.pie(shadow=True, startangle=170,autopct='%.2f')

"""### We could see that We have more Undergraduates 41.92% than graduates(28.06%) & Advanced Professional(30.02%)"""

def SD_CD(row):
    if (row['Securities Account']==1) & (row['CD Account']==1):
        return"Holds Securities & deposit"
    elif(row['Securities Account']==0) & (row['CD Account']==0):
        return"Does not hold any securities or deposit"
    elif(row['Securities Account']==1) & (row['CD Account']==0):
        return "Holds only Securities Account"
    elif(row['Securities Account']==0) & (row['CD Account']==1):
        return"Holds only deposit"

df['Account_Holder_Category']=df.apply(SD_CD,axis=1)
df['Account_Holder_Category'].value_counts().plot.pie(shadow=True, startangle=125,autopct='%.2f')

"""#### We could see that alomst 87% of customers do not hold any securities or deposit, and 3 % hold both securities as well as deposit. It will be good if we encourage those 87% to open any of these account as it will improve the assests of the bank"""

sns.boxplot(df['Education'],df['Income'],hue=df['Personal Loan'])

plt.figure(figsize=(12,8))
sns.distplot(df[df['Personal Loan']==0]['Income'],kde=True,color='r',hist=False,label="Income distribution for customers with no personal Loan")
sns.distplot(df[df['Personal Loan']==1]['Income'],kde=True,color='G',hist=False,label="Income distribution for customers with personal Loan")
plt.legend()
plt.title("Income Distribution")

"""#### Customers who have availed personal loan seem to have higher income than those who do not have personal loan"""

plt.figure(figsize=(12,8))
sns.distplot(df[df['Personal Loan']==0]['CCAvg'],kde=True,hist=False,color='r',label="Credit card average for customers with no personal Loan")
sns.distplot(df[df['Personal Loan']==1]['CCAvg'],kde=True,hist=False,color='G',label="Credit card average for customers with personal Loan")
plt.legend()
plt.title("CCAvg Distribution")

plt.figure(figsize=(12,8))
sns.distplot(df[df['Personal Loan']==0]['Mortgage'],kde=True,hist=False,color='r',label="Mortgage of customers with no personal Loan")
sns.distplot(df[df['Personal Loan']==1]['Mortgage'],kde=True,hist=False,color='G',label="Mortgage of customers with personal Loan")
plt.legend()
plt.title("Mortgage Distribution")

"""#### People with high mortgage value, i.e more than 400K have availed personal Loan"""

plt.figure(figsize=(14,12))
sns.countplot(df['Account_Holder_Category'],hue=df['Personal Loan'],palette='Blues')
plt.show();

"""#### From the above graph we could infer that , customers who hold deposit account & customers who do not hold either a securities account or deposit account have aviled personal loan"""

Data=df.drop(['EDU','Account_Holder_Category'],axis=1)

Data.head()

import scipy.stats as stats
sns.scatterplot(Data['Age'],Data['Personal Loan'],hue=Data['Family'],alpha=0.8)

H0="Age does not have any impact on availing personal Loan"
Ha="Age does have phenomenal significance on availing personal Loan"

Age_PL_Yes=np.array(Data[Data['Personal Loan']==1].Age)
Age_PL_No=np.array(Data[Data['Personal Loan']==0].Age)

t,p_value=stats.ttest_ind(Age_PL_Yes,Age_PL_No,axis=0)

if p_value < 0.05:
    print(Ha,"As the P_value is less than 0.05 with a value of :{}".format(p_value))
else:
    print (H0,"As the P_value is Greater than 0.05 with a value of :{}".format(p_value))

sns.scatterplot(Data['Age'],Data['Personal Loan'],hue=Data['Income'],alpha=0.8)
Income_PL_Yes=np.array((Data[Data['Personal Loan']==1]).Income)
Income_PL_No=np.array((Data[Data['Personal Loan']==0]).Income)

H0="Income of a person does not have an impact on availing Personal Loan"
Ha="Income of a person has significant impact on availing Personal Loan"

t,p_value=stats.ttest_ind(Income_PL_Yes,Income_PL_No,axis=0)

if p_value < 0.05:
    print(Ha,"As the P_value is less than 0.05 with a value of :{}".format(p_value))
    print("As you can see from the plot, those who availed Personal Loan tend to have higher income")
else:
    print (H0,"As the P_value is Greater than 0.05 with a value of :{}".format(p_value))

sns.scatterplot(Data['Age'],Data['Personal Loan'],hue=Data['Family'],alpha=0.8)
Family_PL_Yes=np.array((Data[Data['Personal Loan']==1]).Family)
Family_PL_No=np.array((Data[Data['Personal Loan']==0]).Family)

H0="Number of persons in the family does not have an impact on availing Personal Loan"
Ha="Number of persons in the family has significant impact on availing Personal Loan"

t,p_value=stats.ttest_ind(Family_PL_Yes,Family_PL_No,axis=0)

if p_value < 0.05:
    print(Ha,"As the P_value is less than 0.05 with a value of :{}".format(p_value))
else:
    print (H0,"As the P_value is Greater than 0.05 with a value of :{}".format(p_value))

print(Data['Personal Loan'].value_counts())
No_of_customers_availed_PL=Data[Data['Personal Loan']==1].shape[0]
No_of_customers_availed_PL
Total_Cust=Data.shape[0]
percet=(No_of_customers_availed_PL * 100)/Total_Cust 
print("Overall percentage of customers who have availed personal Loan:{}".format(percet),"%")

"""#### As you could see, our **Target Variable** is not equally distributed, only 9.6% of customers have availed Personal Loan. So, if our model is going to learn from this dataset and do the prediction chances are there that it might be biased towards the Majority class (In this case , Personal loan not being availed by the customer) and ignore the minority class. Hence , we should try to balance our dataset to make our model learn and predict with being biased and treat both classes equally for better result."""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score

X=Data.drop(['Personal Loan'],axis=1)
y=Data['Personal Loan']

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=23)

Model1_raw=LogisticRegression(solver='liblinear')
Model1_raw.fit(X_train,y_train)

Model1_raw_coef=pd.DataFrame(Model1_raw.coef_)
Model1_raw_coef

Model1_raw.score(X_train,y_train)

Model1_raw.score(X_test,y_test)

Model1_raw_prediction=Model1_raw.predict(X_test)

"""### Accuracy,F1, Precision & recall with imbalanced data for Logistic Regression"""

cm_model1=confusion_matrix(y_test,Model1_raw_prediction,labels=[0,1])
print(cm_model1)
acc_score_log1=accuracy_score(y_test,Model1_raw_prediction)
f1_score_log1=f1_score(y_test,Model1_raw_prediction)
print("Accuracy Score  for Logistic Regression RAW DATA:{}".format(acc_score_log1*100))
print("F1 Score  for Logistic Regression RAW DATA:{}".format(f1_score_log1*100))
print(classification_report(y_test,Model1_raw_prediction))

"""#### Though we have a very good accuracy score of 94%, We should not consider this score as our data is highly imbalanced."""

from  sklearn.utils import resample
df_majority=Data[Data['Personal Loan']==0]
df_minority=Data[Data['Personal Loan']==1]
df_upsample_minority=resample(df_minority,replace=True,random_state=12,n_samples=4520)
df_upsample=pd.concat([df_majority,df_upsample_minority])
count=df_upsample['Personal Loan'].value_counts()
count.plot(kind='bar',figsize=(5,5));

"""#### Now class imbalance has been rectified , as you can see from above both the classes for our Dependent variable "Personal Loan" are same, we have done minority upsampling to have equal distribution of both the classes."""

X_upsampled=df_upsample.drop(['Personal Loan'],axis=1)
Y_upsampled=df_upsample['Personal Loan']

X_upsampled_test,X_upsampled_train,Y_upsampled_test,Y_upsampled_train=train_test_split(X_upsampled,Y_upsampled,random_state=1,test_size=0.3)

Model1_raw.fit(X_upsampled_train,Y_upsampled_train)

Pred_Upsample_Log=Model1_raw.predict(X_upsampled_test)
cm_model1_upsample=confusion_matrix(Y_upsampled_test,Pred_Upsample_Log,labels=[0,1])

print(classification_report(Y_upsampled_test,Pred_Upsample_Log))

"""#### As , you can see the upsampled data produced a accouracy score of 91%, with 93% recall and 89% Precision. Whereas, Raw data had 54% recall and 84% Precision.

### **Naive Bayes** with both Raw & Upsampled Data
"""

from sklearn.naive_bayes import GaussianNB
Model2_nb=GaussianNB()

Model2_nb.fit(X_upsampled_train,Y_upsampled_train)
Model2_nb.fit(X_train,y_train)

Model2_nb.fit(X_upsampled_test,Y_upsampled_test)
Model2_nb.fit(X_test,y_test)

print("####################Raw Data Score####################")
print(Model2_nb.score(X_train,y_train))
print(Model2_nb.score(X_test,y_test))
print("####################Sample Data Score####################")
print(Model2_nb.score(X_upsampled_train,Y_upsampled_train))
print(Model2_nb.score(X_upsampled_test,Y_upsampled_test))

Pred_nb_raw=Model2_nb.predict(X_test)
Pred_nb_Upsampled=Model2_nb.predict(X_upsampled_test)
CM_NB_RAW=confusion_matrix(y_test,Pred_nb_raw)
CM_NB_UPSAMPLE=confusion_matrix(Y_upsampled_test,Pred_nb_Upsampled)

acc_score_NB_Raw=accuracy_score(y_test,Pred_nb_raw)
f1_score_NB_raw=f1_score(y_test,Pred_nb_raw)
print("Accuracy Score  for Naive Bayes Model RAW DATA:{}".format(acc_score_NB_Raw*100))
print("F1 Score  for Naive Bayes Model RAW DATA:{}".format(f1_score_NB_raw*100))
print("+++++++++++++++THE CONFUSION MATRIX RAW DATA++++++++++++++++")
print("Confusion Matrix: \n",CM_NB_RAW)
print("+++++++++++++++CLASSIFICATION REPORT RAW DATA++++++++++++++++")
print(classification_report(y_test,Pred_nb_raw))
acc_score_NB_upsampled=accuracy_score(Y_upsampled_test,Pred_nb_Upsampled)
f1_score_NB_upsampled=f1_score(Y_upsampled_test,Pred_nb_Upsampled)
print("Accuracy Score  for Naive Bayes Model Upsampled Data:{}".format(acc_score_NB_upsampled*100))
print("F1 Score  for Naive Bayes Model:{}".format(f1_score_NB_upsampled*100))
print("+++++++++++++++THE CONFUSION MATRIX Upsampled Data++++++++++++++++")
print("Confusion Matrix: \n",CM_NB_UPSAMPLE)
print("+++++++++++++++CLASSIFICATION REPORT Upsampled Data++++++++++++++++")
print(classification_report(Y_upsampled_test,Pred_Upsample_Log))

"""#### As , you can see the upsampled data produced a accouracy score of 74%, with 93% recall and 89% Precision. Whereas, Raw data had 57% recall and 44% Precision which was low. Though accuracy was high other metrics with raw data on Naive Bayes were not effective

### **KNN** Algorithm with Raw and Upsampled Data
"""

from sklearn.neighbors import KNeighborsClassifier

Model3_KNN=KNeighborsClassifier(n_neighbors=11)
Model3_KNN.fit(X_train,y_train)

Predict_knn=Model3_KNN.predict(X_test)
CM_KNN=confusion_matrix(y_test,Predict_knn)

Model3_KNN.fit(X_upsampled_train,Y_upsampled_train)
Predict_KNN_Sampled=Model3_KNN.predict(X_upsampled_test)
CM_KNN_SAMPLED=confusion_matrix(Y_upsampled_test,Predict_KNN_Sampled)

acc_score_KNN_Raw=accuracy_score(y_test,Predict_knn)
f1_score_KNN_raw=f1_score(y_test,Predict_knn)
print("Accuracy Score  for KNN with RAW DATA:{}".format(acc_score_KNN_Raw*100))
print("F1 Score for KNN with RAW DATA:{}".format(f1_score_KNN_raw*100))
print("+++++++++++++++THE CONFUSION MATRIX for KNN RAW DATA++++++++++++++++")
print("Confusion Matrix: \n",CM_KNN)
print("+++++++++++++++CLASSIFICATION REPORT for KNN RAW DATA++++++++++++++++")
print(classification_report(y_test,Predict_knn))
acc_score_KNN_upsampled=accuracy_score(Y_upsampled_test,Predict_KNN_Sampled)
f1_score_KNN_upsampled=f1_score(Y_upsampled_test,Predict_KNN_Sampled)
print("Accuracy Score  for KNN with Upsampled Data:{}".format(acc_score_KNN_upsampled*100))
print("F1 Score  for for KNN with Upsampled Data:{}".format(f1_score_KNN_upsampled*100))
print("+++++++++++++++THE CONFUSION MATRIX KNN Upsampled Data++++++++++++++++")
print("Confusion Matrix: \n",CM_KNN_SAMPLED)
print("+++++++++++++++CLASSIFICATION REPORT KNN Upsampled Data++++++++++++++++")
print(classification_report(Y_upsampled_test,Predict_KNN_Sampled))

"""#### As , you can see the upsampled data produced a accouracy score of 87%, with 94% recall and 83% Precision. Whereas, Raw data had 26% recall and 60% Precision.

### **Model evaluation**
"""

from sklearn.metrics import roc_curve,auc,roc_auc_score


PRED_PROB_LOG_RAW=Model1_raw.predict_proba(X_test)
PRED_PROB_LOG_SAMPLED=Model1_raw.predict_proba(X_upsampled_test)

PRED_PROB_NB_RAW=Model2_nb.predict_proba(X_test)
PRED_PROB_NB_SAMPLED=Model2_nb.predict_proba(X_upsampled_test)

PRED_PROB_KNN_RAW=Model3_KNN.predict_proba(X_test)
PRED_PROB_KNN_SAMPLED=Model3_KNN.predict_proba(X_upsampled_test)

#calculate fpr,tpr,threshold
fpr1, tpr1, thresh1 = roc_curve(y_test, PRED_PROB_LOG_RAW[:,1], pos_label=1)
fpr2,tpr2,thresh2= roc_curve(Y_upsampled_test,PRED_PROB_LOG_SAMPLED[:,1],pos_label=1)
fpr3,tpr3,thresh3=roc_curve(y_test,PRED_PROB_NB_RAW[:,1],pos_label=1)
fpr4,tpr4,thresh4=roc_curve(Y_upsampled_test,PRED_PROB_NB_SAMPLED[:,1],pos_label=1)
fpr5,tpr5,thresh5=roc_curve(y_test,PRED_PROB_KNN_RAW[:,1],pos_label=1)
fpr6,tpr6,thresh6=roc_curve(Y_upsampled_test,PRED_PROB_KNN_SAMPLED[:,1],pos_label=1)


random_probs = [0 for i in range(len(y_test))]
p_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)


AUC_LOG_RAW=roc_auc_score(y_test,PRED_PROB_LOG_RAW[:,1])
AUC_LOG_SAMPLED=roc_auc_score(Y_upsampled_test,PRED_PROB_LOG_SAMPLED[:,1])
AUC_NB_RAW=roc_auc_score(y_test,PRED_PROB_NB_RAW[:,1])
AUC_NB_UPSAMPLED=roc_auc_score(Y_upsampled_test,PRED_PROB_NB_SAMPLED[:,1])
AUC_KNN_RAW=roc_auc_score(y_test,PRED_PROB_KNN_RAW[:,1])
AUC_KNN_UPSAMPLED=roc_auc_score(Y_upsampled_test,PRED_PROB_KNN_SAMPLED[:,1])

AUC_SCORES=pd.array([AUC_LOG_RAW,AUC_LOG_SAMPLED,AUC_NB_RAW,AUC_NB_UPSAMPLED,AUC_KNN_RAW,AUC_KNN_UPSAMPLED])



#Plot Area Under Curve

plt.plot(fpr1,tpr1,linestyle='--',color='orange', label='Logistic Regression RAW')
plt.plot(fpr2,tpr2,linestyle='solid',color='blue', label='Logistic Regression Sampled')
plt.plot(fpr3,tpr3,linestyle='--',color='Green', label='Naive Bayes RAW')
plt.plot(fpr4,tpr4,linestyle='solid',color='Red', label='Naive Bayes Sampled')
plt.plot(fpr5,tpr5,linestyle='--',color='violet',label='KNN RAW')
plt.plot(fpr6,tpr6,linestyle='solid',color='black',label='KNN Upsampled')

plt.plot(p_fpr, p_tpr, linestyle=':', color='red')
# title
plt.title('ROC curve')
# x label
plt.xlabel('False Positive Rate')
# y label
plt.ylabel('True Positive rate')

plt.legend(loc='best')
plt.savefig('ROC',dpi=300)
plt.show();

print("ROC_AUC_Score for Logistic Regression with Raw Data:{}".format(AUC_LOG_RAW))
print("ROC_AUC_Score for Logistic Regression with Upsample  Data:{}".format(AUC_LOG_SAMPLED))
print("ROC_AUC_Score for Naive Bayes with Raw Data:{}".format(AUC_NB_RAW))
print("ROC_AUC_Score for Naive Bayes with Upsample  Data:{}".format(AUC_NB_UPSAMPLED))
print("ROC_AUC_Score for KNN with Raw Data:{}".format(AUC_KNN_RAW))
print("ROC_AUC_Score for KNN with Upsample  Data:{}".format(AUC_KNN_UPSAMPLED))  
print("=======================================================================")
print("The Best AUC_SCORE that we have got is :{}".format(AUC_SCORES.max()))

"""## **Conclusion**

#### Higher the AUC, better the model is at distinguishing between a customer buying personal Loan and Not buying Personal Loan.

### The Blue line shows that our **Logistic model** on sampled data alomst covers more region and 96% can predict our class covering both cutomers who will buy and not buy Personal Loan.
"""

